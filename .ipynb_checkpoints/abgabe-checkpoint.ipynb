{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abgabe Data Exploration Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl Datensatz\n",
    "\n",
    "Für das Data Exploration Projekt wurde ein Datensatz gewählt, der sich mit Immobilienverkäufen im Raum Washington, USA, befasst. Dieser Datensatz umfasst Daten zu verschiedenen Aspekten des Immobilienmarktes, einschließlich Verkaufspreise, Anzahl der Schlafzimmer und Badezimmer, Wohnfläche in Quadratfuß, Grundstücksgröße, Etagenanzahl, Vorhandensein einer Wasserfront, Aussicht, Zustand der Immobilie, Baujahr, Wohnfläche oberhalb der Erde, Kellerfläche, Jahr der Renovierung, Straßenadresse, Stadt, Postleitzahl und Land. Die Auswahl fiel auf diesen Datensatz, da er eine breite Palette an Features bietet, die für das Verständnis der Dynamik und Preisgestaltung auf dem Immobilienmarkt relevant sind. Zudem spiegelt der Handel mit Immobilien einen stets aktuellen und dynamischen Markt wider, was die Relevanz dieses Datensatzes für das Verständnis von Markttrends und Preisgestaltungsmechanismen unterstreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benötigten Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn as sns\n",
    "import scipy.stats as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import os\n",
    "import time\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "from folium.plugins import MarkerCluster\n",
    "df = pd.read_csv(\"Daten/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charakterisierung des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Datensatz erstmal auf Null-Werte überprüft, die später die Berechnungen verfälschen könnten. Wie jedoch zu sehen ist, enthält jede spalte genau 4600 Einträge was auf keine leeren Zeilen hinweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zählt eindeutige Werte je Spalte.\n",
    "df.nunique(axis = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfung auf Duplikate\n",
    "duplicate_count = df.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Anzahl der Duplikate im Datensatz: {duplicate_count}\")\n",
    "else:\n",
    "    print(\"Keine Duplikate gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugt und transponiert deskriptive Statistiken der numerischen Spalten.\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basierend auf der dargestellten Auswertung gibt es mehrere Auffälligkeiten und potenzielle Probleme im Datensatz, die weitere Untersuchungen erfordern:\n",
    "\n",
    "    - Preis von 0 \n",
    "    - Schlafzimmer und Badezimmer von 0\n",
    "    - Sehr hoher maximaler Preis ( über 26 mio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zählen und Ausgeben der Anzahl von Einträgen mit einem Hauspreis von 0\n",
    "zero_price_count        = (df.price     == 0.0).sum()\n",
    "zero_bedrooms_count     = (df.bedrooms  == 0.0).sum()\n",
    "zero_bathrooms_count    = (df.bathrooms == 0.0).sum()\n",
    "\n",
    "print(f\"Anzahl der Einträge mit einem Hauspreis von 0: {zero_price_count}\")\n",
    "print(f\"Anzahl der Einträge mit einem Schlafzimmer von 0: {zero_bedrooms_count}\")\n",
    "print(f\"Anzahl der Einträge mit einem Badezimmer von 0: {zero_bathrooms_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ersetzt alle 0-Werte in der 'price', 'bedrooms', 'bathrooms'-Spalte durch NaN\n",
    "df['price']       = df['price'].replace(0, np.nan)\n",
    "df['bedrooms']    = df['bedrooms'].replace(0, np.nan)\n",
    "df['bathrooms']   = df['bathrooms'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "# Ermittelt und gibt die Anzahl der fehlenden Werte (NaNs) \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernt Zeilen mit NaN Einträge\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "zero_price_count_after_cleaning     = (df.price == 0).sum()\n",
    "zero_bedrooms_count_after_cleaning  = (df.bedrooms == 0).sum()\n",
    "zero_bathrooms_count_after_cleaning = (df.bathrooms == 0).sum()\n",
    "\n",
    "print(f\"Anzahl der Einträge mit einem Hauspreis von 0 nach der Bereinigung: {zero_price_count_after_cleaning}\")\n",
    "print(f\"Anzahl der Einträge mit einem Schlafzimmer von 0 nach der Bereinigung: {zero_bedrooms_count_after_cleaning}\")\n",
    "print(f\"Anzahl der Einträge mit einem Badezimmer von 0 nach der Bereinigung: {zero_bathrooms_count_after_cleaning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Datensätze mit fehlenden Werten entfernt wurden, kann nun gezielt nach der Immobilie mit dem außergewöhnlich hohen Preis gesucht werden. Um alle Ausreißer zu identifizieren, wird der Interquartilsabstand (IQR) herangezogen. Aufgrund der ungleichen Verteilung einzelner Merkmale im Datensatz, wird ein besonders aggressiver Multiplikator von 5 verwendet. Dieser Ansatz zielt darauf ab, ausschließlich die extremsten Ausreißer zu erfassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR für den Preis pro Quadratfuß berechnen\n",
    "df['price_per_sqft'] = df['price'] / df['sqft_living']\n",
    "\n",
    "\n",
    "# Berechnung des Interquartilsabstands (IQR) für den Preis pro Quadratfuß\n",
    "Q1 = df['price_per_sqft'].quantile(0.25)\n",
    "Q3 = df['price_per_sqft'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Festlegung der Grenzen für die Ausreißer\n",
    "lower_bound = Q1 - 5 * IQR  #Mulitplikator sehr agressive gewählt, um nicht zu viel als Ausreißer zu makieren\n",
    "upper_bound = Q3 + 5 * IQR\n",
    "\n",
    "outliers = df[(df['price_per_sqft'] < lower_bound) | (df['price_per_sqft'] > upper_bound)]\n",
    "\n",
    "# Speichern der Ausreißer in einem neuen DataFrame\n",
    "outliers_df = outliers.copy()\n",
    "\n",
    "# Anzahl der Ausreißer anzeigen\n",
    "print(f\"Anzahl der Ausreißer im Preis pro Quadratfuß: {outliers_df.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aller Daten\n",
    "pyplot.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='sqft_living', y='price', data=df, color='lightgray', alpha=0.6)\n",
    "\n",
    "# Hervorheben der Ausreißer\n",
    "sns.scatterplot(x='sqft_living', y='price', data=outliers, color='red')\n",
    "pyplot.title('Preis in Abhängigkeit von der Wohnfläche mit Ausreißern')\n",
    "pyplot.xlabel('Wohnfläche (sqft)')\n",
    "pyplot.ylabel('Preis')\n",
    "pyplot.legend(['Daten', 'Ausreißer'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Analyse wurden 5 Ausreißer identifiziert, was auf eine bedeutende Streuung im Vergleich zum Durchschnitt des Datensatzes hindeutet. Diese befinden sich deutlich über der Mehrheit der Daten, was auf außergewöhnlich teure Immobilien hindeuten könnte. Diese Ausreißer verdienen eine genauere Untersuchung, um zu verstehen, ob sie aufgrund einzigartiger Eigenschaften oder möglicherweise aufgrund von Eingabefehlern zu diesen extremen Preispunkten geführt haben. \n",
    "\n",
    "Nächsten Schritte:\n",
    "\n",
    "    - Detaillierte Untersuchung der Ausreißer\n",
    "    - Entscheidung über die Behandlung der Ausreißer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "outliers_df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analyse führte zum Ausschluss des Hauses mit der ID 4350 aus dem Datensatz, da sein Preis unverhältnismäßig hoch ist im Vergleich zu den Häusern ähnlicher Größe, Ausstattung und neuerem Baujahr. Das Haus mit der ID 4346 wird ebenfalls entfernt, weil sein Preis sich angesichts ähnlicher Merkmale zu anderen Häusern nicht rechtfertigen lässt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(index=[4350, 4346])\n",
    "df = df.drop(df[df['price'] == 7800].index) #wird entfernt, Fehler da unrealistischer Preis\n",
    "\n",
    "\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestimmen, wie viele Subplots pro Zeile angezeigt werden sollen\n",
    "subplots_per_row = 3\n",
    "\n",
    "# Features kategorisieren\n",
    "continuous_features = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "categorical_features = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Berechnen, wie viele Reihen benötigt werden\n",
    "num_rows_continuous = -(-len(continuous_features) // subplots_per_row)\n",
    "num_rows_categorical = -(-len(categorical_features) // subplots_per_row)\n",
    "\n",
    "# Erstellen der Subplots für kontinuierliche Features\n",
    "fig, axs = pyplot.subplots(num_rows_continuous, subplots_per_row, figsize=(15, num_rows_continuous * 4))\n",
    "axs = axs.flatten()  # Flachmachen der Achsen für einfachen iterativen Zugang\n",
    "for i, feature in enumerate(continuous_features):\n",
    "    sns.histplot(df[feature], kde=False, bins=30, ax=axs[i])\n",
    "    axs[i].set_title(f'Verteilung des Features: {feature}')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('Anzahl der Immobilien')\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()\n",
    "\n",
    "# Erstellen der Subplots für kategoriale Features\n",
    "fig, axs = pyplot.subplots(num_rows_categorical, subplots_per_row, figsize=(15, num_rows_categorical * 4))\n",
    "axs = axs.flatten()\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.countplot(x=feature, data=df, ax=axs[i])\n",
    "    axs[i].set_title(f'Verteilung des Features: {feature}')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('Anzahl der Immobilien')\n",
    "    axs[i].tick_params(axis='x', rotation=90)\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Dominanz von Familienhäusern mit 3-4 Schlafzimmern im Datensatz deutet darauf hin, dass unser Algorithmus in der Vorhersage solcher Standardimmobilien effektiver sein wird, während er bei ungewöhnlichen Immobilienarten weniger genau sein könnte. Um die Vorhersagegenauigkeit über ein breiteres Spektrum von Immobilientypen zu erhöhen, könnten Anpassungen wie Stratified Sampling und Feature-Skalierung notwendig sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Korrelationsmatrix\n",
    "# Entferne nicht-numerische Spalten\n",
    "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Berechnung der Korrelationsmatrix für numerische Daten\n",
    "corr_matrix_numerical = numerical_df.corr()\n",
    "\n",
    "# Erstellung der Heatmap für die numerische Korrelationsmatrix\n",
    "pyplot.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix_numerical, annot=True, fmt=\".2f\", cmap='YlGnBu')\n",
    "pyplot.title('Heatmap der Feature-Korrelationen für numerische Daten')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wohnfläche (sqft_living), oberirdische Wohnfläche (sqft_above), Kellerfläche (sqft_basement), Anzahl der Badezimmer (bathrooms) und Schlafzimmer (bedrooms) sind wegen ihrer starken Korrelation mit dem Immobilienpreis Schlüsselelemente für die Preisvorhersage. Waterfront und view könnten weniger Gewicht im Modell erhalten, da ihre Korrelation mit dem Preis schwächer ist. Features mit geringer Korrelation sollten möglicherweise entfernt werden, um das Modell zu straffen und Überanpassung zu vermeiden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge ein Feature für die Gesamtanzahl der Räume hinzu\n",
    "df['total_rooms'] = df['bedrooms'] + df['bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne das aktuelle Jahr\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Berechne das Alter des Hauses seit Bau\n",
    "df['age_since_built'] = current_year - df['yr_built']\n",
    "\n",
    "# Berechne das Alter des Hauses seit der letzten Renovierung\n",
    "# Falls das Haus nie renoviert wurde (yr_renovated = 0), verwende das Baujahr\n",
    "df['age_since_renovated'] = df.apply(lambda row: current_year - row['yr_renovated'] if row['yr_renovated'] > 0 else row['age_since_built'], axis=1)\n",
    "\n",
    "# Zeige die ersten Zeilen des DataFrame zur Überprüfung\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere den MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Liste der zu normalisierenden Features\n",
    "features_to_normalize = ['sqft_living', 'sqft_above', 'sqft_basement', 'sqft_lot']\n",
    "\n",
    "# Anwenden des Scalers auf die ausgewählten Features und Ersetzen der ursprünglichen Werte\n",
    "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n",
    "\n",
    "# Überprüfung: Zeige die ersten Zeilen des DataFrame, um die Änderungen zu sehen\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eins der wichtigsten Eigenschaften um den Wert einer Immobilie zu ermitteln ist der Lage der Immobilie. Hierfür werden die Adressen mithilfe einer API in Längen und Breitengrade umgewandelt, damit der Algorythmus später damit arbeiten kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinieren der Spalten 'street', 'city' und 'statezip' in eine neue Spalte 'Adress'\n",
    "df['Adress'] = df['street'] + ', ' + df['city'] + ', ' + df['statezip']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere den Geocoder\n",
    "#geolocator = Nominatim(user_agent=\"Test\")\n",
    "#geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, error_wait_seconds=10)\n",
    "\n",
    "# Überprüfe, ob die test.csv Datei existiert und lade sie gegebenenfalls\n",
    "#existing_entries = {}\n",
    "#if os.path.exists('Daten/test.csv'):\n",
    "#    existing_df = pd.read_csv('Daten/test.csv')\n",
    "    # Erstelle ein Dictionary der existierenden Einträge für schnellen Zugriff\n",
    "#    existing_entries = {row['Adress']: (row['Latitude'], row['Longitude']) for index, row in existing_df.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])}\n",
    "#else:\n",
    "#    print(\"Keine existierende 'test.csv' gefunden.\")\n",
    "\n",
    "# Funktion zum Speichern einer Zeile in die CSV-Datei\n",
    "#def save_row_to_csv(row, file_path, mode='a'):\n",
    "#    row.to_csv(file_path, mode=mode, header=not os.path.exists(file_path), index=False)\n",
    "\n",
    "# Start der Verarbeitung und sofortiges Speichern\n",
    "#for index, row in df.iterrows():\n",
    "#    address = row['Adress']\n",
    "#    if address not in existing_entries:\n",
    "#        try:\n",
    "#            # Führe Geokodierung durch\n",
    "#            location = geocode(address)\n",
    "#            if location:\n",
    "#               latitude, longitude = location.latitude, location.longitude\n",
    "#                print(f\"Umwandlung erfolgreich: {address} -> {latitude}, {longitude}\")\n",
    "#                # Speichere die aktuelle Zeile direkt in die CSV-Datei\n",
    "#                save_row_to_csv(pd.DataFrame([[address, latitude, longitude]], columns=['Adress', 'Latitude', 'Longitude']), 'Daten/test.csv')\n",
    "#            else:\n",
    "#                print(f\"Kein Ergebnis für: {address}\")\n",
    "#        except Exception as e:\n",
    "#            print(f\"Fehler bei der Geokodierung für: {address} - {e}\")\n",
    "#        # Sicherstellen, dass wir die API-Beschränkungen einhalten\n",
    "#        time.sleep(1)\n",
    "#    else:\n",
    "#        print(f\"Eintrag bereits vorhanden: {address}\")\n",
    "#\n",
    "#print(\"Die Datei wurde erfolgreich mit allen Zeilen erstellt und aktualisiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code wurde dringelassen der Vollständigkeitshalber. In diesem Verfahren wurde das ergebniss in eine Nue CSV-Datei gespeichert, damit alles nachvollziehbar ist und da das Ausführen dieses Codes über eine Stunde bracuht um die Ratelimits der Webseite zu garantieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Long_lat_df = pd.read_csv('Daten/test.csv')\n",
    "\n",
    "df = pd.merge(df, Long_lat_df[['Adress', 'Latitude', 'Longitude']], on='Adress', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = df[df['Latitude'].isna() | df['Longitude'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze OpenCage API-Schlüssel\n",
    "#key = 'b0948bca3b374fdbbcccf1a54d3abcc0'\n",
    "#geocoder = OpenCageGeocode(key)\n",
    "#\n",
    "# Überprüfe, ob die LangLat.csv Datei existiert und lade sie gegebenenfalls\n",
    "#existing_entries = {}\n",
    "#if os.path.exists('Daten/LangLat.csv'):\n",
    "#    existing_df = pd.read_csv('Daten/LangLat.csv')\n",
    "#    # Erstelle ein Dictionary der existierenden Einträge für schnellen Zugriff\n",
    "#    existing_entries = {row['Adress']: (row['Latitude'], row['Longitude']) for index, row in existing_df.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])}\n",
    "#else:\n",
    "#    print(\"Keine existierende 'LangLat.csv' gefunden.\")\n",
    "#\n",
    "# Funktion zum Speichern einer Zeile in die CSV-Datei\n",
    "#def save_row_to_csv(row, file_path, mode='a'):\n",
    "#    row.to_csv(file_path, mode=mode, header=not os.path.exists(file_path), index=False)\n",
    "#\n",
    "# Start der Verarbeitung und sofortiges Speichern\n",
    "#for index, row in nan_df.iterrows():\n",
    "#    address = row['Adress']\n",
    "#    if address not in existing_entries:\n",
    "#        try:\n",
    "#            # Führe Geokodierung durch\n",
    "#            result = geocoder.geocode(address)\n",
    "#            if result and result[0]['geometry']:\n",
    "#                latitude = result[0]['geometry']['lat']\n",
    "#                longitude = result[0]['geometry']['lng']\n",
    "#                print(f\"Geokodierung erfolgreich: {address} -> {latitude}, {longitude}\")\n",
    "#                # Speichere die aktuelle Zeile direkt in die CSV-Datei\n",
    "#                save_row_to_csv(pd.DataFrame([[address, latitude, longitude]], columns=['Adress', 'Latitude', 'Longitude']), 'Daten/LangLat.csv')\n",
    "#            else:\n",
    "#                print(f\"Kein Ergebnis für: {address}\")\n",
    "#        except Exception as e:\n",
    "#            print(f\"Fehler bei der Geokodierung für: {address} - {e}\")\n",
    "#        # Verzögerung, um die Rate-Limits zu respektieren\n",
    "#        time.sleep(1)\n",
    "#    else:\n",
    "#        print(f\"Eintrag bereits vorhanden: {address}\")\n",
    "#\n",
    "#print(\"Die Datei 'LangLat.csv' wurde erfolgreich mit allen Zeilen erstellt und aktualisiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wurde das Selbe wie mit geopy gemacht nur dieses mal mit OpenCageGeocode, da Geopy bei den anderen Adressen Probleme hatte, diese umzuwandeln. Hier wurde auch der API-Key drin gelassen, was eigentlich nicht den Best-Practices entspricht, falls der Code überprüft werden sollte damit Sie kein Account erstellen müsse. Zudem können keine Kosten druchdie Nutzung entstehen das es auch 2.500 Request pro Tag limitiert ist und somit gratis ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_long_lat_df = pd.read_csv('Daten/LangLat.csv')\n",
    "\n",
    "# Führe einen Merge durch, der die Geodaten basierend auf der Übereinstimmung der Adressen aktualisiert\n",
    "df_updated = pd.merge(df, rest_long_lat_df[['Adress', 'Latitude', 'Longitude']], on='Adress', how='left', suffixes=('', '_update'))\n",
    "\n",
    "# Aktualisiere die ursprünglichen Latitude und Longitude Werte in df mit den aktualisierten Werten, falls vorhanden\n",
    "df['Latitude'] = df_updated['Latitude_update'].combine_first(df_updated['Latitude'])\n",
    "df['Longitude'] = df_updated['Longitude_update'].combine_first(df_updated['Longitude'])\n",
    "\n",
    "# Entferne die temporären Update-Spalten\n",
    "df.drop(columns=['Latitude_update', 'Longitude_update'], errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errechne die logarithmierten Preise für die Farbskala\n",
    "prices_log = np.log(df['price'])  \n",
    "max_price_log = prices_log.max()\n",
    "min_price_log = prices_log.min()\n",
    "\n",
    "# Erstelle eine Farbskala basierend auf logarithmierten Preisen\n",
    "color_scale = LinearColormap(['green', 'yellow', 'red'], vmin=min_price_log, vmax=max_price_log)\n",
    "\n",
    "# Funktion, die den logarithmierten Preis eines Hauses in eine Farbe umwandelt\n",
    "def price_to_color(log_price):\n",
    "    return color_scale(log_price)\n",
    "\n",
    "# Erstelle eine Karte, die auf die mittleren Koordinaten deiner Daten zentriert ist\n",
    "map_center = [df['Latitude'].mean(), df['Longitude'].mean()]  \n",
    "washington_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Füge CircleMarker hinzu, wobei die Farbe vom logarithmierten Preis abhängt\n",
    "for index, row in df.iterrows():  \n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=2,\n",
    "        color=price_to_color(np.log(row['price'])),\n",
    "        fill=True,\n",
    "        fill_color=price_to_color(np.log(row['price'])),\n",
    "        fill_opacity=0.8\n",
    "    ).add_to(washington_map)\n",
    "\n",
    "# Füge die Farbskala als Legende hinzu\n",
    "color_scale.caption = 'Hauspreis (logarithmisch)'\n",
    "color_scale.add_to(washington_map)\n",
    "\n",
    "# Zeige die Karte an\n",
    "washington_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer neuen Karte, zentriert um den Durchschnitt der Koordinaten\n",
    "map_center = [df['Latitude'].mean(), df['Longitude'].mean()]\n",
    "map_clustering = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Erstellen eines MarkerClusters\n",
    "marker_cluster = MarkerCluster().add_to(map_clustering)\n",
    "\n",
    "# Farbskala basierend auf logarithmierten Preisen\n",
    "color_scale = LinearColormap(['green', 'yellow', 'red'], vmin=min_price_log, vmax=max_price_log)\n",
    "\n",
    "# Funktion, die den logarithmierten Preis eines Hauses in eine Farbe umwandelt\n",
    "def price_to_color(price):\n",
    "    return color_scale(np.log(price))\n",
    "\n",
    "# Hinzufügen von Markern zum Cluster\n",
    "for index, row in df.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        icon=folium.Icon(color=price_to_color(row['price'])),\n",
    "        popup=f'${row[\"price\"]:,.0f}'\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Hinzufügen der Farbskala als Legende hinzu\n",
    "color_scale.caption = 'Hauspreis (logarithmisch)'\n",
    "color_scale.add_to(map_clustering)\n",
    "\n",
    "# Anzeigen der Karte\n",
    "map_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['price_per_sqft', 'street', 'city', 'statezip','country' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Korrelationsmatrix\n",
    "# Entferne nicht-numerische Spalten\n",
    "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Entferne die Spalten 'Latitude' und 'Longitude' aus dem DataFrame\n",
    "numerical_df = numerical_df.drop(['Latitude', 'Longitude'], axis=1)\n",
    "\n",
    "# Berechnung der Korrelationsmatrix für numerische Daten\n",
    "corr_matrix_numerical = numerical_df.corr()\n",
    "\n",
    "# Erstellung der Heatmap für die numerische Korrelationsmatrix\n",
    "pyplot.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix_numerical, annot=True, fmt=\".2f\", cmap='YlGnBu')\n",
    "pyplot.title('Heatmap der Feature-Korrelationen für numerische Daten')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split des Datensatzes\n",
    "\n",
    "\n",
    "Ich abe mich für eine Aufteilung von 70% Training, 15% Validierung und 15% Test entschieden. Dies bietet eine ausgewogene Basis: genug Trainingsdaten für den Modellentwurf, eine Validierungsmenge für Hyperparameter-Tuning und Modellselektion, sowie eine Testmenge für eine unvoreingenommene Leistungsevaluation. Der random_state gewährleistet konsistente und wiederholbare Aufteilungen für vergleichbare Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trennt Merkmale (X) und Zielvariable (y).\n",
    "X = df.drop([\"price\"], axis=1)\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Teilt Daten in Trainings-, Validierungs- und Testsätze.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombiniert Trainingssmerkmale und -ziel.\n",
    "train_data = X_train.join(y_train)\n",
    "\n",
    "# Kombiniert Validierungsmerkmale und -ziel.\n",
    "val_data = X_val.join(y_val)\n",
    "\n",
    "# Kombiniert Trainingsmerkmale und -ziel.\n",
    "test_data = X_test.join(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trainingsdaten: {len(train_data)} Zeilen\")\n",
    "print(f\"Validierungsdaten: {len(val_data)} Zeilen\")\n",
    "print(f\"Testdaten: {len(test_data)} Zeilen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
